#Verificar tweets vac√≠os o con pocas palabras

import pandas as pd
import joblib
from utils.text_processing import preprocess_text
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re



cleaned_tweets = joblib.load("/home/mario/Documents/camiApp/data/CleanAndEmbeddins/cleaned_tweets.pkl")

#Chequeo de tweets con pocas palabras
short_tweets = [tweet for tweet in cleaned_tweets if len(tweet.split()) <= 1]
print(f"üîç Tweets con 1 palabras o menos: {len(short_tweets)}")
print(short_tweets[:10])  # Muestra algunos ejemplos

cleaned_tweets = [tweet for tweet in cleaned_tweets if len(tweet.split()) > 1] # Elimina tweets con 1 palabra o menos



#Revisar la distribuci√≥n de longitud de tweets despu√©s de la limpieza
tweet_lengths = [len(tweet.split()) for tweet in cleaned_tweets]

plt.hist(tweet_lengths, bins=30, edgecolor='black')
plt.xlabel('N√∫mero de palabras')
plt.ylabel('Frecuencia')
plt.title('Distribuci√≥n de longitud de tweets')
plt.show()

print(f"üìä Longitud promedio: {sum(tweet_lengths)/len(tweet_lengths):.2f} palabras")
print(f"üìä Longitud m√°xima: {max(tweet_lengths)} palabras")
print(f"üìä Longitud m√≠nima: {min(tweet_lengths)} palabras")


#Revisar palabras m√°s frecuentes (nube de palabras)
all_text = " ".join(cleaned_tweets)
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()  #‚úÖ Si hay palabras irrelevantes, podr√≠amos ajustar la lista de STOPWORDS.


#Buscar caracteres raros o s√≠mbolos que no deber√≠an estar presentes
suspicious_tweets = [tweet for tweet in cleaned_tweets if re.search(r"[^a-zA-Z0-9\s]", tweet)]
print(f"üîç Tweets con caracteres raros: {len(suspicious_tweets)}")
print(suspicious_tweets[:10])  # Mostrar algunos ejemplos. ‚úÖ Si hay muchos, se puede ajustar la funci√≥n remove_special_characters().


 #Validar que no haya duplicados
print(f"üõ† Tweets √∫nicos: {len(set(cleaned_tweets))} de {len(cleaned_tweets)} totales") #‚úÖ Tweets √∫nicos: 9,844 de 9,983, lo que significa que hab√≠a 139 duplicados (1.4%), que es un n√∫mero bajo.

#‚úÖ Si hay muchos repetidos, podemos eliminarlos con: cleaned_tweets = list(set(cleaned_tweets)).



# ========================
# Guardado de datos
# ========================

# Guardar solo la columna de tweets limpios en el archivo .pkl
output_path = "/home/mario/Documents/camiApp/data/CleanAndEmbeddins/cleaned_tweets.pkl"
joblib.dump(cleaned_tweets, output_path)

# Cargar y verificar los datos guardados
loaded_tweets = joblib.load(output_path)
print(f"‚úÖ Tweets guardados correctamente: {len(loaded_tweets)} tweets")
print(f"üîç Ejemplo de tweets limpios: {loaded_tweets[:5]}")  # Mostrar algunos ejemplos